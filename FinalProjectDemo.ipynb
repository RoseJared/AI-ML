{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIwHzbIqMnF91B32lUAU86",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoseJared/AI-ML/blob/main/FinalProjectDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Demonstation Exerpts\n",
        "This code runs the Top 2 models determined in the full code:\n",
        "- Random Forset Regression\n",
        "- Gradient Boosting Regression\n",
        "\n",
        "This code runs in **4 min 22 sec**.\n",
        "\n",
        "### Importing Visualization Libraries\n",
        "\n",
        "In this section, we are importing two popular Python libraries used for creating visualizations:\n",
        "\n",
        "- **`matplotlib.pyplot`**: This is a core plotting library that allows us to create charts like line graphs, bar charts, and scatter plots. We import it as `plt` to make it easier to use in our code.\n",
        "- **`seaborn`**: This is a library built on top of `matplotlib` that makes it easier to create more visually appealing and informative charts, especially when working with data in tables (like pandas DataFrames).\n",
        "\n",
        "We'll use these tools later to explore the data and better understand the relationships between different features (columns) in our dataset.\n"
      ],
      "metadata": {
        "id": "a_3gTxPJL2OE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHUrbQe6LwKg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVR\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint, uniform\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Datasets\n",
        "\n",
        "In this section, we are using the **`pandas`** library (imported as `pd`) to load two datasets from Google Drive:\n",
        "\n",
        "- **`beijing_df`**: This dataset contains air quality data specifically for Beijing. It likely includes information such as pollutant levels, weather conditions, and timestamps.\n",
        "- **`worldcities_df`**: This dataset contains information about cities around the world, such as their names, countries, and geographic coordinates (latitude and longitude).\n",
        "\n",
        "To load these files:\n",
        "1. We start with a shared Google Drive URL for each file.\n",
        "2. We modify the URL so that it can be accessed directly by Python using `pandas.read_csv()`, which reads the data into a structured table called a **DataFrame**.\n",
        "\n",
        "DataFrames are useful because they allow us to easily view, analyze, and manipulate large tables of data using Python.\n"
      ],
      "metadata": {
        "id": "Jhxmk6L4L8oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url= 'https://drive.google.com/file/d/1BFefZwgVG5eBv0f4ocR2tfWKINJ7XzWb/view?usp=sharing'\n",
        "url= 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "beijing_df= pd.read_csv(url)"
      ],
      "metadata": {
        "id": "dcXZjrDQMAK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Data\n",
        "\n",
        "In this step, we clean the dataset by filling in missing values:\n",
        "\n",
        "1. **Filling Missing Values in Numerical Columns**:\n",
        "   - For each numerical feature (like `PM2.5`, `TEMP`, `RAIN`, etc.), we replace missing values with the **median** of that column.\n",
        "   - The median is used because it's less affected by extreme values (outliers) than the average, making it a reliable choice for filling in missing data.\n",
        "\n",
        "2. **Filling Missing Values in Categorical Column**:\n",
        "   - For the `wd` column (which stands for wind direction and contains text values), we fill in missing entries using the **most frequent value** (also known as the mode).\n",
        "\n",
        "3. **Verifying the Fixes**:\n",
        "   - After filling in the missing values, we check again to confirm that there are no missing values left in the dataset.\n",
        "\n",
        "Cleaning missing data is an essential step before building any machine learning model, as many algorithms cannot handle empty or incomplete values."
      ],
      "metadata": {
        "id": "B1LZ92d5MXeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill numerical columns with their medians\n",
        "numerical_cols = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']\n",
        "for col in numerical_cols:\n",
        "    beijing_df[col] = beijing_df[col].fillna(beijing_df[col].median())\n",
        "\n",
        "# Fill wind direction (categorical) with most common value\n",
        "beijing_df['wd'] = beijing_df['wd'].fillna(beijing_df['wd'].mode()[0])\n"
      ],
      "metadata": {
        "id": "4mZVr4WvMP6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating the Air Quality Index (AQI)\n",
        "\n",
        "In this section, we compute the Air Quality Index (AQI) for each row in the dataset based on standard pollutant concentration levels:\n",
        "\n",
        "1. **Unit Conversion**:\n",
        "   - The `CO` and `O3` columns are converted from micrograms per cubic meter (μg/m³) to more standard units: parts per million (ppm) for CO and parts per billion (ppb) for O3.\n",
        "\n",
        "2. **AQI Breakpoints**:\n",
        "   - Each pollutant has defined concentration ranges (called breakpoints) that map to corresponding AQI values. These are based on official environmental health standards.\n",
        "\n",
        "3. **AQI Calculation**:\n",
        "   - For each row of data, we calculate the AQI for all six pollutants and keep the highest one. This represents the overall AQI for that time and location.\n",
        "\n",
        "4. **AQI Category Labeling**:\n",
        "   - The final AQI value is mapped to a category like \"Good\", \"Moderate\", or \"Unhealthy\", which helps communicate the quality of the air in a way that’s easier to understand.\n",
        "\n",
        "5. **Result Preview**:\n",
        "   - A few key columns (station, date/time, AQI, and category) are shown to confirm the AQI calculation worked correctly.\n",
        "\n",
        "This process transforms raw pollutant measurements into a single, easy-to-understand indicator of air quality.\n"
      ],
      "metadata": {
        "id": "6QmTpDfkMmpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correcting CO and O3 to appropriate units\n",
        "#beijing_df['CO'] = beijing_df['CO'] / 1000  # Convert from μg/m³ to ppm\n",
        "#beijing_df['O3'] = beijing_df['O3'] / 1000  # Convert from μg/m³ to ppb\n",
        "#internet https://www.freeonlinecalc.com/air-quality-index-aqi-calculation-review-and-formulas.html\n",
        "\n",
        "# Convert CO from µg/m³ to ppm\n",
        "beijing_df['CO'] = beijing_df['CO'] * (24.45 / 28010)\n",
        "\n",
        "# Convert O3 from µg/m³ to ppb\n",
        "beijing_df['O3'] = (beijing_df['O3'] * 24.45 / (48 * 1000)) * 1000\n",
        "# Define AQI breakpoints for each pollutant\n",
        "aqi_breakpoints = {\n",
        "    'PM2.5': [(0, 12), (12.1, 35.4), (35.5, 55.4), (55.5, 150.4), (150.5, 250.4), (250.5, 500.4)],\n",
        "    'PM10': [(0, 54), (55, 154), (155, 254), (255, 354), (355, 424), (425, 604)],\n",
        "    'SO2': [(0, 35), (36, 75), (76, 185), (186, 304), (305, 604), (605, 1004)],\n",
        "    'NO2': [(0, 53), (54, 100), (101, 360), (361, 649), (650, 1249), (1250, 2049)],\n",
        "    'CO': [(0.0, 4.4), (4.5, 9.4), (9.5, 12.4), (12.5, 15.4), (15.5, 30.4), (30.5, 50.4)],\n",
        "    'O3': [(0, 54), (55, 70), (71, 85), (86, 105), (106, 200), (201, 404)]\n",
        "}\n",
        "\n",
        "# Corresponding AQI index ranges\n",
        "aqi_indices = [(0, 50), (51, 100), (101, 150), (151, 200), (201, 300), (301, 500)]\n",
        "\n",
        "# Function to calculate individual AQI for a pollutant\n",
        "def calculate_individual_aqi(concentration, breakpoints, aqi_range):\n",
        "    for i in range(len(breakpoints)):\n",
        "        low_bp, high_bp = breakpoints[i]\n",
        "        low_idx, high_idx = aqi_range[i]\n",
        "        if low_bp <= concentration <= high_bp:\n",
        "            aqi = ((high_idx - low_idx) / (high_bp - low_bp)) * (concentration - low_bp) + low_idx\n",
        "            return round(aqi)\n",
        "    return 500  # Return 500 if beyond highest range\n",
        "\n",
        "# Function to calculate AQI for each row\n",
        "def calculate_aqi(row):\n",
        "    aqi_list = []\n",
        "    for pollutant in aqi_breakpoints.keys():\n",
        "        concentration = row[pollutant]\n",
        "        breakpoints = aqi_breakpoints[pollutant]\n",
        "        aqi_list.append(calculate_individual_aqi(concentration, breakpoints, aqi_indices))\n",
        "    return max(aqi_list)\n",
        "\n",
        "# Apply AQI calculation to each row\n",
        "beijing_df['AQI'] = beijing_df.apply(calculate_aqi, axis=1)\n",
        "\n",
        "# Map AQI to AQI Category\n",
        "def categorize_aqi(aqi):\n",
        "    if aqi <= 50:\n",
        "        return 'Good'\n",
        "    elif aqi <= 100:\n",
        "        return 'Moderate'\n",
        "    elif aqi <= 150:\n",
        "        return 'Unhealthy for Sensitive Groups'\n",
        "    elif aqi <= 200:\n",
        "        return 'Unhealthy'\n",
        "    elif aqi <= 300:\n",
        "        return 'Very Unhealthy'\n",
        "    else:\n",
        "        return 'Hazardous'\n",
        "\n",
        "beijing_df['AQI_Category'] = beijing_df['AQI'].apply(categorize_aqi)\n",
        "\n",
        "# Preview the resulting DataFrame\n",
        "print(beijing_df[['station', 'year', 'month', 'day', 'hour',\n",
        "                  'PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3',\n",
        "                  'AQI', 'AQI_Category']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY3yJ0EDMoc3",
        "outputId": "4770a9eb-fa63-4dbb-e93c-5385fd67e29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              station  year  month  day  hour  PM2.5  PM10   SO2   NO2  \\\n",
            "0        Aotizhongxin  2013      3    1     0    4.0   4.0   4.0   7.0   \n",
            "1        Aotizhongxin  2013      3    1     1    8.0   8.0   4.0   7.0   \n",
            "2        Aotizhongxin  2013      3    1     2    7.0   7.0   5.0  10.0   \n",
            "3        Aotizhongxin  2013      3    1     3    6.0   6.0  11.0  11.0   \n",
            "4        Aotizhongxin  2013      3    1     4    3.0   3.0  12.0  12.0   \n",
            "...               ...   ...    ...  ...   ...    ...   ...   ...   ...   \n",
            "420763  Wanshouxigong  2017      2   28    19   11.0  32.0   3.0  24.0   \n",
            "420764  Wanshouxigong  2017      2   28    20   13.0  32.0   3.0  41.0   \n",
            "420765  Wanshouxigong  2017      2   28    21   14.0  28.0   4.0  38.0   \n",
            "420766  Wanshouxigong  2017      2   28    22   12.0  23.0   4.0  30.0   \n",
            "420767  Wanshouxigong  2017      2   28    23   13.0  19.0   4.0  38.0   \n",
            "\n",
            "              CO         O3  AQI AQI_Category  \n",
            "0       0.261871  39.221875   36         Good  \n",
            "1       0.261871  39.221875   36         Good  \n",
            "2       0.261871  37.184375   34         Good  \n",
            "3       0.261871  36.675000   34         Good  \n",
            "4       0.261871  36.675000   34         Good  \n",
            "...          ...        ...  ...          ...  \n",
            "420763  0.349161  36.675000   46         Good  \n",
            "420764  0.436451  25.468750   53     Moderate  \n",
            "420765  0.436451  27.506250   55     Moderate  \n",
            "420766  0.349161  30.053125   50         Good  \n",
            "420767  0.523742  24.959375   53     Moderate  \n",
            "\n",
            "[420768 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = {\n",
        "    'PM2.5': 5.0,  # Expected AQI = Good\n",
        "    'PM10': 50.0,  # Expected AQI = Good\n",
        "    'SO2': 30.0,   # Expected AQI = Good\n",
        "    'NO2': 50.0,   # Expected AQI = Good\n",
        "    'CO': 1.0,     # Expected AQI = Good\n",
        "    'O3': 30.0     # Expected AQI = Good\n",
        "}\n",
        "\n",
        "test_df = pd.DataFrame([test_data])\n",
        "test_df['AQI'] = test_df.apply(calculate_aqi, axis=1)\n",
        "test_df['AQI_Category'] = test_df['AQI'].apply(categorize_aqi)\n",
        "\n",
        "print(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJJHNW5BMs5r",
        "outputId": "3819dc1c-292c-4f81-bcc5-e6738f55198e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PM2.5  PM10   SO2   NO2   CO    O3  AQI AQI_Category\n",
            "0    5.0  50.0  30.0  50.0  1.0  30.0   47         Good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Data for AQI Prediction\n",
        "\n",
        "This section sets up the data for building a machine learning model that can predict AQI based on environmental measurements:\n",
        "\n",
        "1. **Library Imports**:\n",
        "   - We import tools from the `scikit-learn` library for splitting data, building a linear regression model, evaluating performance, and scaling features.\n",
        "\n",
        "2. **Feature Selection**:\n",
        "   - We choose a set of predictor variables (features) that likely affect air quality, such as pollutant levels (`PM2.5`, `CO`, etc.) and weather data (`TEMP`, `PRES`).\n",
        "   - The variable we want to predict is `AQI`, which serves as our **target**.\n",
        "\n",
        "3. **Preparing Input and Output**:\n",
        "   - `X` holds the features (input data), and `y` holds the target (AQI values).\n",
        "\n",
        "4. **Train-Test Split**:\n",
        "   - The data is split into two parts, standard ratio used:\n",
        "     - **Training set** (80%): Used to train the model.\n",
        "     - **Testing set** (20%): Used to evaluate how well the model performs on new, unseen data.\n",
        "\n",
        "Splitting the data this way helps ensure that the model can generalize well and is not just memorizing the training data.\n"
      ],
      "metadata": {
        "id": "fC-si647M26O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'beijing_df' is your DataFrame\n",
        "# Select relevant features for AQI prediction\n",
        "features = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']\n",
        "#'TEMP', 'PRES'\n",
        "target = 'AQI'\n",
        "\n",
        "# Prepare the features and target\n",
        "X = beijing_df[features]\n",
        "y = beijing_df[target]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (optional but often helps with performance)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "gMXPyiRMM12P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Regression Model\n",
        "\n",
        "We train a Random Forest Regressor to predict AQI:\n",
        "\n",
        "- This model builds an ensemble of decision trees and averages their predictions for more accurate results.\n",
        "- It is trained on the original (unscaled) training data.\n",
        "- Random Forest is an ensemble method that builds multiple decision trees and averages their predictions.\n",
        "- The model is trained with 100 trees (n_estimators=100) using the training data.\n",
        "- After training, predictions are made on the test set.\n",
        "- The model's performance is measured using Mean Squared Error (MSE) and R² score.\n",
        "\n",
        "This model often captures complex patterns better than linear models.\n",
        "\n",
        "#### How Random Forest Differs from a Decision Tree\n",
        "\n",
        "- A Decision Tree is a single model that splits data into branches based on feature values to make predictions. It is simple, easy to interpret, but can overfit to the training data.\n",
        "\n",
        "- A Random Forest is an ensemble of many decision trees. Each tree is trained on a random subset of the data and features. The final prediction is the average of all tree predictions (for regression).\n"
      ],
      "metadata": {
        "id": "b1i3Y2MINJtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Random Forest model\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Random Forest - MSE: {mse_rf}, R²: {r2_rf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paetQELbNHt8",
        "outputId": "c00515dd-51f5-4fad-a284-8141a7c7f4fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - MSE: 89.57417025096846, R²: 0.9886476132453791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting Regression\n",
        "\n",
        "We train a Gradient Boosting Regressor to predict AQI:\n",
        "\n",
        "- This model builds an ensemble of trees, each correcting errors from the previous one.\n",
        "- It's known for high accuracy on structured data.\n",
        "\n",
        "**Evaluation**:\n",
        "- MSE and R² scores show how well the model performs on the test set.\n",
        "\n",
        "Gradient Boosting is a strong performer and often used in competitions and production systems.\n"
      ],
      "metadata": {
        "id": "8ayG-QwTNWyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Best one rerun\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Train Gradient Boosting Regressor\n",
        "gb_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "gb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_gb = gb_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the Gradient Boosting model\n",
        "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
        "r2_gb = r2_score(y_test, y_pred_gb)\n",
        "print(f\"Gradient Boosting - MSE: {mse_gb}, R²: {r2_gb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEAseFSHNVSt",
        "outputId": "97e9f88c-d4d8-45b6-cfd2-76870f4ff890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting - MSE: 70.99888625782124, R²: 0.9910017942260826\n"
          ]
        }
      ]
    }
  ]
}